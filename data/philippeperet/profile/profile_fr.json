{
  "person": {
    "lastname": "peret",
    "firstname": "philippe",
    "location": "Paris, France",
    "phone": "+33.6.10.48.30.14",
    "email": "philippe.peret@hotmail.com"
  },
  "title": {
    "main": "chef de projet data senior",
    "subtitle": "expert data technico-functionnel"
  },
  "links": [
    {"name": "linkedin", "icon": "file", "href": "https://www.linkedin.com/in/philippeperet/", "label": "Voir mon profil LinkedIn"}
    ],
  "summary": [
    {"p": "Depuis 2001, j'ai bâti mon expérience sur des projets data et des rôles technico-fonctionnels autour du Data Management, de la qualité et la transformation des données ainsi que la BI."},
    {"p": "J'apprécie les pipelines de données complexes et des tableaux de bord soigneusement conçus."}
    ],
  "takeaways": {
    "Compétences spécifiques": [
        {"item": "Collecter et représenter les besoins métiers"},
        {"item": "Approche data Full-Stack"},
        {"item": "Parler le même language que les équipes techniques"},
        {"item": "Agilité & résolution des problèmes"}
      ],
    "Stack & outils clés" : [
        {"item": "R / Python, API REST"},
        {"item": "SQL, Hive, Cloudera"},
        {"item": "Power.BI"},
        {"item": "Data Galaxy"},
        {"item": "Azure, GCP"}
      ],
    "Domaines Spécifiques": [
        {"item": "Transports & logistique"},
        {"item": "RSE"},
        {"item": "Références clients : PSA, BMW, Airbus, Alstom, Miele, Claas"}
      ],
    "Formation & Certifications": [
        {"item": "La plus récente : Generative AI with LLMs"},
        {"item": "Data Science (Université Johns Hopkins)"},
        {"item": "Ingénieur ESTACA (Automobile)"}
      ],
    "Management": [
        {"item": "35 personnes"},
        {"item": "Contexte intl. (France, Inde, USA)"},
        {"item": "Transition, développement de carrière"}
      ],
    "Langues": [
        {"item": "Français (natif)"},
        {"item": "Anglais (C1)"}
      ]
  },
  "experiences": {
    "geodis_csr": {
      "role": "Chef de Projet Data / RSE",
      "company": "GEODIS",
      "start": "10/2022",
      "end": "12/2023",
      "summary": "Livraison d'un datalake RSE (~150k ingestions / jour)<br>Déploiement d'un dashboard qualité, d'une architecture API et de 2 outils data<br>Participation à l'initiative de Data Gouvernance.",
      "tags": "CSR, Logistic, Datalake, Reporting",
      "pitch": "Within the Corporate Digital & Technology organization and dedicated to the strategic CSR activities of the CO emission calculation & Science Based Target Initiative.",
      "details": {
        "Functional": [
          {"item": "Support CSR & Business Teams with the technical usage of the APIs"},
          {"item": "Monitor data flow & CO2 emission calculation infrastructure"},
          {"item": "Capture technical needs linked to emission calculation (ex: low carbon offers)"},
          {"item": "Follow-up meetings with the teams & external suppliers"}
        ],
        "Project Management": [
          {"item": "Capture the CO2 calculation flow into the Datalake"},
          {"item": "Deliver a dashboard to monitor the quality of the CO2 computations"},
          {"item": "Define and implement a common architecture of the CO2 calculation services"},
          {"item": "Build roadmap & budget, manage priorities, organize steering committees, workshops"}
        ],
        "Data & Analysis": [
          {"item": "Act as a proxy Data Owner, on behalf of the CSR Organization"},
          {"item": "Develop Datalake ingestion validation script"},
          {"item": "Perform data analysis to support the businesses and CSR teams"},
          {"item": "Participate to governance Data Catalog initiative"}
        ]
      },
      "frames": {
        "specificities": [
          {"item": "24 x 7 x 365 data flow"},
          {"item": "High management attention (comex)"}
        ],
        "key figures": "56M CO2 calculations / year, 5 lines of Business, 4 regions, 10 systems connected, 4 computation streams",
        "stack & tools": "Cloudera, HIVE, SQL, Porwer.BI, Porwer.BI, Azure, API, R, Data Galaxy, EcoTransIT",
        "deliverables": {
          "datalake & dashboard": [
            {"item": "CSR Datalake (100k to 300k ingestions daily)"},
            {"item": "CSR Data Quality Dashboard (errors & anomalies)"}
          ],
          "architecture & tools": [
            {"item": "Target Architecture (REST API Portal)"},
            {"item": "2 Tools (Service monitoring, Carbon Compensation Cost)"},
            {"item": "1 API (low carbon offers support)"}
          ],
          "data analysis & governance": [
            {"item": "Shipment calculation flow analysis (800k dataset)"},
            {"item": "Data lineage in Data Galaxy"}
          ]
        }
      }
    },
    "freelance": {
      "role": "Expert Data Technico-fonctionnel",
      "company": "Freelance",
      "start": "01/2020",
      "end": "NA",
      "summary": "Management de Projet Data<br>Mentorat Data Analyse @OpenClassrooms<br>Developpement d'APIs, Scripts & Outils data.",
      "tags": "data quality, data analysis, data visualization",
      "pitch": "As a freelance worker, to maintain a transversal data knowledge that can support both functional & technical sides of complex data projects.",
      "details": {
        "Coaching & Training": [
          {"item": "Mentorship @OpenClassrooms on the Data Analyst program"},
          {"item": "Support students with projects & exams"},
          {"item": "Advocate data culture within business organizations"}
        ],
        "Data pipelines": [
          {"item": "Automate data pipelines (collection, cleaning & transformation)"},
          {"item": "Design & develop APIs"}
        ],
        "Tools & Dashboards": [
          {"item": "Produce data analysis & visualizations"},
          {"item": "Design & develop tools (Dashboards, Maps, Web apps)"},
          {"item": "Develop R packages"}
        ]
      },
      "frames": {
        "specificities": [
          {"item": "R / Shiny expert"},
          {"item": "Full-Stack data approach"}
        ],
        "key figures": "9.8K visits on data.gouv.fr, 1 graduated student, 40 GitHub repos.",
        "stack & tools": "R, Shiny, Python, SQL, Docker, GCP, AWS",
        "deliverables": {
          "GEODIS": [
            {"item": "Datalake validation scripts"},
            {"item": "Monitoring tool (webapp)"},
            {"item": "CO2 computation flow analysis"}
          ],
          "Tools & Apps": [
            {"item": "Election data visualization tool"},
            {"item": "ML monitoring dashboard"}
          ],
          "R Package": [
            {"item": "Tabular data management framework"},
            {"item": "Admin console, demo apps"}
          ]
        }
      }
    },
    "ds_qa": {
      "role": "Responsable QA & Senior Manager",
      "company": "Dassault Systèmes",
      "start": "01/2016",
      "end": "12/2019",
      "summary": "Assurance Qualité d'un portfolio Data Management & Exchanges<br>Livraison d'un framework de tests automatisés (DevOps)<br>Manager et transformer l'organisation (35 pers.).",
      "tags": "data management, data exchange, quality, transformation",
      "pitch": "Within the R&D organization, as a QA Leader for Data Management & Exchanges products and Team Manager.",
      "details": {
        "QA Leader": [
          {"item": "Be responsible for the quality of Data Management & Exchanges products"},
          {"item": "Build and deploy strategies to solve critical program convergence"},
          {"item": "Implement decision support tools to anticipate program drift"},
          {"item": "Plan and deploy test automation strategy (DevOps)"}
        ],
        "Team Transformation": [
          {"item": "Transform mission & roles inside the team (to prepare for automation)"},
          {"item": "Career development (delegate, coach on communication)"},
          {"item": "Turn team & network culture towards a proactive approach"}
        ],
        "Data Project": [
          {"item": "Analyze root causes why objectives and expectations were not met"},
          {"item": "Build and deploy a complete change of strategy"},
          {"item": "Setup a communication process to build support for the project"}
        ]
      },
      "frames": {
        "specificities": [
          {"item": "Team management"},
          {"item": "Transformation"}
        ],
        "key figures": "35 people organization, 5 data products",
        "stack & tools": "ENOVIA, BI, Python, SQL, Sikuli",
        "deliverables": {
          "QA Leader": [
            {"item": "Product & data quality assessments"},
            {"item": "GO/NOGO for product delivery"}
          ],
          "Team & Operations": [
            {"item": "2 Dashboards (Testers / Managers)"},
            {"item": "Methodology and KPIs to detect activity drift"}
          ],
          "Automation": [
            {"item": "QA test automation framework"}
          ]
        }
      }
    },
    "ds_support": {
      "role": "Support Technique (Ingénieur, puis Manager)",
      "company": "Dassault Systèmes",
      "start": "01/2002",
      "end": "01/2015",
      "summary": "Supporter les déploiement de projets clients<br>Converger la qualité de produits Data Management (en tant qu'ingénieur)<br>Converger la qualité de produits Data Exchanges (en tant que Manager).",
      "tags": "data management, data quality, data exchange, deployment",
      "pitch": "Within the R&D Technical Customer Support, dedicated to Data Management (as an Engineer), then Data Transition & Exchanges products (as a Manager) and assigned to production deployment projects.",
      "details": {
        "Functional": [
          {"item": "Create customer references"},
          {"item": "Participate in customer workshops to collect business needs"},
          {"item": "Manage backlog to ensure technical convergence"},
          {"item": "Participate, then lead a transversal task force to drive product convergence efforts"}
        ],
        "Project & Deployment": [
          {"item": "Participate, then lead follow-up meetings (internal and client)"},
          {"item": "Manage customer critical situations"},
          {"item": "Build convergence & recovery plans"},
          {"item": "Advocate customer & user views to the development teams"}
        ],
        "Data Management": [
          {"item": "Implement data extraction, preparation & processing (transfer & migration)"},
          {"item": "Setup data processing strategies"},
          {"item": "Ensure data quality & lifecycle"},
          {"item": "Analyze and improve performances of the data pipelines"}
        ]
      },
      "frames": {
        "specificities": [
          {"item": "Product convergence"},
          {"item": "High management attention (comex)"}
        ],
        "key figures": "7 years as Engineer, 7 years as Manager",
        "stack & tools": "ENOVIA, Oracle, SQL",
        "deliverables": {
          "Products": [
            {"item": "Technical convergence (maturity)"},
            {"item": "Documentation of best practices"}
          ],
          "Data Exchanges": [
            {"item": "3 references (Alstom, Claas, Miele)"},
            {"item": "Annual dashboard (areas for improvement)"}
          ],
          "Data Management": [
            {"item": "1 reference customer (BMW)"},
            {"item": "Weekly code delivery"}
          ]
        }
      }
    }
  }
}